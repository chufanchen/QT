# @package _global_
defaults:
  - agent_params: ../../agent_params/erqt
  - env_params: ../../env_params/hopper_medium_expert
  - run_params: ../../run_params/base
  - _self_
  - override /hydra/sweeper: optuna
  - override /hydra/launcher: ray

hydra:
  launcher:
    ray:
      init:
        num_gpus: 4
        local_mode: false
        _temp_dir: /home/data_hdd/ray_tmp
      remote:
        num_gpus: 1
        max_calls: 1
  sweeper:
    sampler:
      seed: 123
    direction: maximize
    study_name: hopper_medium_expert_optimization
    storage: null  # Use in-memory storage
    n_trials: 200
    n_jobs: 8      # Number of parallel jobs
    params:
      run_params.learning_rate: choice(0.0003, 0.003)
      run_params.lr_decay: choice(true, false)
      run_params.eta: choice(0.4, 1.0, 5.0)
      run_params.eta1: choice(0.02, 0.000005)
      run_params.alpha: choice(0.01, 0.1, 0.5, 1.0, 10.0)
  sweep:
    dir: multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}

seed: 123
device: cuda
log_to_wandb: true
save_path: ./save/
exp_name: hpo

wandb_params:
  project: erqt
  monitor_gym: True
  save_code: True