# @package _global_
defaults:
  - _self_
  - override /hydra/sweeper: optuna
  - override /hydra/launcher: ray

hydra:
  launcher:
    ray:
      init:
        num_gpus: 4
        local_mode: false
      remote:
        num_gpus: 1
        max_calls: 1
  sweeper:
    sampler:
      seed: 123
    direction: maximize
    study_name: halfcheetah_medium_replay_optimization
    storage: null  # Use in-memory storage
    n_jobs: 1      # Number of parallel jobs
    params:
      K: choice(5, 10, 20)
      batch_size: choice(32, 64, 128)
      learning_rate: choice(0.0001, 0.0003, 0.0005)
      lr_decay: choice(true, false)
      eta: choice(0.4, 1.0, 5.0)
      alpha: choice(0.01, 0.1, 0.5)
      seed: choice(0, 123, 456, 789)
  sweep:
    dir: multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}

K: 5
batch_size: 32
grad_norm: 15.0
seed: 0

# Fixed parameters
model_type: qdt
mode: normal
n_layer: 3
embed_dim: 128
n_head: 4
activation_function: relu
dropout: 0.1
learning_rate: 0.0001
lr_decay: true
lr_min: 0.0
weight_decay: 1e-4

warmup_steps: 10000
num_eval_episodes: 10
max_iters: 500
num_steps_per_iter: 10000



env: halfcheetah
dataset: medium-replay
use_aug: true
pct_traj: 0.1
dataset_postfix: null
create_pct_traj_and_exit: false

discount: 0.99
use_discount: true
k_rewards: true
reward_tune: no
sar: false
scale: null
test_scale: null
rtg_no_q: false
infer_no_q: false

tau: 0.005
alpha: 0.01
eta: 1.0
eta2: 1.0
lambda: 1.0
lambda1: 1.0
max_q_backup: true

early_stop: true
early_epoch: 100


log_to_wandb: true
exp_name: test
save_path: ./save/

w: true
wandb_param:
  project: erqt
  tags: ["sweep", "halfcheetah"]

stochastic_policy: true
policy_penalty: true
value_penalty: false
behavior_ckpt_file: "./save/10%_bc_stochastic-halfcheetah-medium-replay-123-250324-112957/epoch_15.pth"